{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79afa5b-30d6-4a7d-9c1f-68a466c6d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from typing import List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204bb525-4d32-4448-957e-d36fc560b560",
   "metadata": {},
   "source": [
    "Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9b73d5-00c8-4223-8477-e07035abfc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sets_path = 'D:\\\\datasets\\\\VOC2012\\\\ImageSets\\\\Main\\\\'\n",
    "train_path = os.path.join(image_sets_path, 'train.txt')\n",
    "val_path = os.path.join(image_sets_path, 'val.txt')\n",
    "\n",
    "images_path = 'D:\\\\datasets\\\\VOC2012\\\\JPEGImages\\\\'\n",
    "annots_path = 'D:\\\\datasets\\\\VOC2012\\\\Annotations\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247600c7-e853-4329-b288-b0fd8b172853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5717\n",
      "val: 5823\n"
     ]
    }
   ],
   "source": [
    "train_images = utils.read_set(train_path)\n",
    "val_images = utils.read_set(val_path)\n",
    "\n",
    "set(train_images) & set(val_images)\n",
    "\n",
    "print(f'train: {len(train_images)}')\n",
    "print(f'val: {len(val_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02e324-1330-469a-83db-7ffba0eae012",
   "metadata": {},
   "source": [
    "Загрузка базовой модели PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d76260c-456d-42a3-9112-490578e44e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cce4a9f-424d-476a-973e-78f3ab17d32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd3b63058c042509e3363754ce6d8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  {'map': tensor(0.5615), 'map_50': tensor(0.7291), 'map_75': tensor(0.6085), 'map_small': tensor(0.1909), 'map_medium': tensor(0.4111), 'map_large': tensor(0.6619), 'mar_1': tensor(0.4643), 'mar_10': tensor(0.6253), 'mar_100': tensor(0.6278), 'mar_small': tensor(0.2524), 'mar_medium': tensor(0.4810), 'mar_large': tensor(0.7129), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19], dtype=torch.int32)}\n",
      "SEC PER FRAME:  0.014\n"
     ]
    }
   ],
   "source": [
    "res_map, mean_sec_per_frame = utils.evaluate_over_voc(\n",
    "    images_path=images_path,\n",
    "    annots_path=annots_path,\n",
    "    val_images=val_images,\n",
    "    model=model,\n",
    "    proc=processor\n",
    ")\n",
    "\n",
    "print('MAP: ', res_map)\n",
    "print('SEC PER FRAME: ', mean_sec_per_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73156366-1b16-4473-a3c8-f989f1a3c0d7",
   "metadata": {},
   "source": [
    "Конвертация модели в ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9ced9c-a8cb-4cfd-9b07-f6635795cef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sidelnikovns\\AppData\\Local\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:626: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (batch_size * self.num_heads, target_len, source_len):\n",
      "C:\\Users\\sidelnikovns\\AppData\\Local\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:633: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (batch_size, 1, target_len, source_len):\n",
      "C:\\Users\\sidelnikovns\\AppData\\Local\\anaconda3\\envs\\gpu\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:657: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (batch_size * self.num_heads, target_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "batch_size = 1\n",
    "\n",
    "#check if model is fine\n",
    "input = torch.randn(batch_size, 3, 800, 1137, requires_grad=True)\n",
    "# torch_out = model(input)\n",
    "# print(torch_out)\n",
    "\n",
    "torch.onnx.export(model, input, \"detr-resnet-50.onnx\",\n",
    "                  export_params=True, opset_version=11,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = [\"pixel_values\", \"pixel_mask\"],\n",
    "                  output_names = ['output']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e98d15-85ae-487a-b223-0593b6643138",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"detr-resnet-50.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c165cd8-9bcc-4398-b552-7fcff15a0d9d",
   "metadata": {},
   "source": [
    "Тестирование модели ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "244082df-f11b-48fe-b1ca-4015fcaf31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "print(ort.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de6cb1c6-bceb-4112-b5bc-c769653bfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = ort.InferenceSession('detr-resnet-50.onnx', providers=['CUDAExecutionProvider']) # providers=['CUDAExecutionProvider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c555132a-7937-463f-8b19-38518b23ce16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b14b7ee7942428bb6b7239982e6a78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  {'map': tensor(0.5399), 'map_50': tensor(0.7018), 'map_75': tensor(0.5839), 'map_small': tensor(0.1740), 'map_medium': tensor(0.4010), 'map_large': tensor(0.6355), 'mar_1': tensor(0.4496), 'mar_10': tensor(0.6052), 'mar_100': tensor(0.6075), 'mar_small': tensor(0.2333), 'mar_medium': tensor(0.4679), 'mar_large': tensor(0.6890), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19], dtype=torch.int32)}\n",
      "SEC PER FRAME:  0.036\n"
     ]
    }
   ],
   "source": [
    "res_map, mean_sec_per_frame = utils.evaluate_over_voc_onnx(\n",
    "    images_path=images_path,\n",
    "    annots_path=annots_path,\n",
    "    val_images=val_images,\n",
    "    model=ort_session,\n",
    "    torch_model=model,\n",
    "    proc=processor,\n",
    ")\n",
    "\n",
    "print('MAP: ', res_map)\n",
    "print('SEC PER FRAME: ', mean_sec_per_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec4b55-9273-4c80-bf31-af1c993ab889",
   "metadata": {},
   "source": [
    "Конвертация PyTorch модели в OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fe5d4-08e7-4803-af48-d2170e7e5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не удачно...\n",
    "\n",
    "ov_model = ov.convert_model(model)\n",
    "ov.save_model(ov_model, \"detr-resnet-50-from-torch-static.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d42d39-3d87-4d45-bfb8-917293d9f6df",
   "metadata": {},
   "source": [
    "Конвертация ONNX модели в OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ac94c3-fff7-49f8-8ecc-74bd71671e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = ov.convert_model(\"detr-resnet-50.onnx\")\n",
    "ov.save_model(ov_model, \"detr-resnet-50.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c7552-08a1-457e-88f0-7a10d678be52",
   "metadata": {},
   "source": [
    "Тестирование модели OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a96e3caa-d187-46d4-b915-a52bc38d86d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 13th Gen Intel(R) Core(TM) i7-13700KF\n",
      "GPU: NVIDIA GeForce RTX 4060 Ti (dGPU)\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "devices = core.available_devices\n",
    "for device in devices:\n",
    "    device_name = core.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ced54d6f-c6e0-4fa2-99ab-d69a3c9d2856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ir = core.read_model(model=\"detr-resnet-50.xml\")\n",
    "compiled_model = core.compile_model(model=model_ir, device_name='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "052d3906-2da8-4a90-ad22-9d1c52c86e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6bbeba6d8540488dbbceb024a92c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  {'map': tensor(0.5385), 'map_50': tensor(0.7016), 'map_75': tensor(0.5823), 'map_small': tensor(0.1722), 'map_medium': tensor(0.3995), 'map_large': tensor(0.6347), 'mar_1': tensor(0.4490), 'mar_10': tensor(0.6041), 'mar_100': tensor(0.6064), 'mar_small': tensor(0.2310), 'mar_medium': tensor(0.4655), 'mar_large': tensor(0.6884), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19], dtype=torch.int32)}\n",
      "SEC PER FRAME:  0.506\n"
     ]
    }
   ],
   "source": [
    "res_map, mean_sec_per_frame = utils.evaluate_over_voc_ov(\n",
    "    images_path=images_path,\n",
    "    annots_path=annots_path,\n",
    "    val_images=val_images,\n",
    "    model=compiled_model,\n",
    "    torch_model=model,\n",
    "    proc=processor,\n",
    ")\n",
    "\n",
    "print('MAP: ', res_map)\n",
    "print('SEC PER FRAME: ', mean_sec_per_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
