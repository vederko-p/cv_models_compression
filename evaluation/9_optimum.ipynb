{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935cbdde-2c3c-4297-84fa-2c78b4c0263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from typing import List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "\n",
    "import my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624dd3c4-67ef-4192-a99b-92d565ff9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sets_path = 'D:\\\\datasets\\\\VOC2012\\\\ImageSets\\\\Main\\\\'\n",
    "train_path = os.path.join(image_sets_path, 'train.txt')\n",
    "val_path = os.path.join(image_sets_path, 'val.txt')\n",
    "\n",
    "images_path = 'D:\\\\datasets\\\\VOC2012\\\\JPEGImages\\\\'\n",
    "annots_path = 'D:\\\\datasets\\\\VOC2012\\\\Annotations\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9027e0a-143b-459d-a381-cce785ef0659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5717\n",
      "val: 5823\n"
     ]
    }
   ],
   "source": [
    "train_images = my_utils.read_set(train_path)\n",
    "val_images = my_utils.read_set(val_path)\n",
    "\n",
    "set(train_images) & set(val_images)\n",
    "\n",
    "print(f'train: {len(train_images)}')\n",
    "print(f'val: {len(val_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c1758a-655f-464d-828b-53cb14187665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "# processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "# model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-small\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfdf9f4-d7c1-445c-81b8-c408b4b52b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca806aee6bf425dbde48e594188fe1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  {'map': tensor(0.5079), 'map_50': tensor(0.6488), 'map_75': tensor(0.5570), 'map_small': tensor(0.3027), 'map_medium': tensor(0.3172), 'map_large': tensor(0.6976), 'mar_1': tensor(0.4670), 'mar_10': tensor(0.5583), 'mar_100': tensor(0.5583), 'mar_small': tensor(0.3676), 'mar_medium': tensor(0.3440), 'mar_large': tensor(0.7267), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19], dtype=torch.int32)}\n",
      "SEC PER FRAME:  1.475\n"
     ]
    }
   ],
   "source": [
    "res_map, mean_sec_per_frame = my_utils.evaluate_over_voc(\n",
    "    images_path=images_path,\n",
    "    annots_path=annots_path,\n",
    "    val_images=val_images[:100],\n",
    "    model=model,\n",
    "    proc=processor,\n",
    ")\n",
    "\n",
    "print('MAP: ', res_map)\n",
    "print('SEC PER FRAME: ', mean_sec_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82eddadf-e2fd-4ae6-9f05-de498b0a4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.bettertransformer import BetterTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fa49f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "model = BetterTransformer.transform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62342d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe27b2d25ff43bfa125eef45878f319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:  {'map': tensor(0.5079), 'map_50': tensor(0.6488), 'map_75': tensor(0.5570), 'map_small': tensor(0.3027), 'map_medium': tensor(0.3172), 'map_large': tensor(0.6976), 'mar_1': tensor(0.4670), 'mar_10': tensor(0.5583), 'mar_100': tensor(0.5583), 'mar_small': tensor(0.3676), 'mar_medium': tensor(0.3440), 'mar_large': tensor(0.7267), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19], dtype=torch.int32)}\n",
      "SEC PER FRAME:  1.098\n"
     ]
    }
   ],
   "source": [
    "res_map, mean_sec_per_frame = my_utils.evaluate_over_voc(\n",
    "    images_path=images_path,\n",
    "    annots_path=annots_path,\n",
    "    val_images=val_images[:100],\n",
    "    model=model,\n",
    "    proc=processor,\n",
    ")\n",
    "\n",
    "print('MAP: ', res_map)\n",
    "print('SEC PER FRAME: ', mean_sec_per_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
